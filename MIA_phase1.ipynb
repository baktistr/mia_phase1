{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "54fd2d0f",
      "metadata": {
        "id": "54fd2d0f"
      },
      "source": [
        "## MIA FOR Finetuned LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a92386",
      "metadata": {
        "id": "89a92386"
      },
      "outputs": [],
      "source": [
        "# update the downloading command as my LFS runs out so cannot directly clone model.safetensors\n",
        "%cd /content\n",
        "\n",
        "!rm -rf /content/Project_phase1\n",
        "\n",
        "# skip LFS!!!: as my LFS bandwith run out\n",
        "!GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/2020pyfcrawl/Project_phase1.git\n",
        "\n",
        "# download the model.safetensors: this enables faster download, you may also upload in colab but it's slower\n",
        "# 1. download the model.safetensors from repo webpage\n",
        "# 2. upload that into your google drive\n",
        "# 3. copy link to share and replace the <...> with your link to download\n",
        "# 4. run the command below\n",
        "# !gdown --fuzzy <your google drive sharing path> -O /content/Project_phase1/models/gpt2_phase1/\n",
        "\n",
        "%cd /content/Project_phase1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check here, model.safetensors should be 475M, if not, delete it and re-download\n",
        "!ls -lah ./models/gpt2_phase1/"
      ],
      "metadata": {
        "id": "nE8MHr08sGfn"
      },
      "id": "nE8MHr08sGfn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c559787",
      "metadata": {
        "id": "0c559787"
      },
      "source": [
        "### Variables and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a53cf5",
      "metadata": {
        "id": "d1a53cf5"
      },
      "outputs": [],
      "source": [
        "# install the required libraries if you have not done so (on you local machine or GPU server)\n",
        "# you may not need to run this if you use colab as they are pre-installed, but you can always do it.\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dab5ec",
      "metadata": {
        "id": "54dab5ec"
      },
      "outputs": [],
      "source": [
        "import os, math, argparse\n",
        "os.environ.setdefault(\"TRANSFORMERS_NO_TORCHVISION\", \"1\")\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_from_disk\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc as _auc\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709e4b5d",
      "metadata": {
        "id": "709e4b5d"
      },
      "outputs": [],
      "source": [
        "# global variable, check the current position to adjust the path\n",
        "target_model_dir = \"./models/gpt2_phase1\"\n",
        "data_dir = \"./data/wiki_json\"\n",
        "batch_size = 25\n",
        "\n",
        "# you may change block size if you like (max length for the tokenizer below)\n",
        "block_size = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef38029",
      "metadata": {
        "id": "fef38029"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f06ed9d",
      "metadata": {
        "id": "7f06ed9d"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(ds, tok, max_len):\n",
        "    ds = ds.filter(lambda ex: ex.get(\"text\", None) and len(ex[\"text\"].strip()) > 0)\n",
        "\n",
        "    def _map(ex):\n",
        "        out = tok(ex[\"text\"], truncation=True, padding=True, max_length=max_len, return_attention_mask=True)\n",
        "        out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "        return out\n",
        "\n",
        "    ds = ds.map(_map, batched=True, remove_columns=ds.column_names)\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    return ds\n",
        "\n",
        "def _read_json(path: Path):\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c040ba66",
      "metadata": {
        "id": "c040ba66"
      },
      "outputs": [],
      "source": [
        "# for tests, you may only load a part of the data to save time while implementing,\n",
        "# as running all 2000 samples on CPU may be slow, but not a problem here for GPU\n",
        "\n",
        "# load test data\n",
        "data_dir = Path(data_dir)\n",
        "test_path = data_dir / \"test.json\"\n",
        "test_items = _read_json(test_path)\n",
        "ds_test = Dataset.from_dict({\"text\": test_items})\n",
        "\n",
        "# tokenizer the test data\n",
        "tokenizer = AutoTokenizer.from_pretrained(target_model_dir, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\"\n",
        "\n",
        "ds_test = tokenize_dataset(ds_test, tokenizer, block_size)\n",
        "dl_test = DataLoader(ds_test, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# you may load the model using the code:\n",
        "# model = AutoModelForCausalLM.from_pretrained(target_model_dir, dtype=\"auto\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(target_model_dir, dtype=\"auto\").to(device)"
      ],
      "metadata": {
        "id": "aCC0VFGmqziA"
      },
      "id": "aCC0VFGmqziA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e72c1fef",
      "metadata": {
        "id": "e72c1fef"
      },
      "source": [
        "### MIA\n",
        "\n",
        "Implement your attack here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8875cce",
      "metadata": {
        "id": "e8875cce"
      },
      "outputs": [],
      "source": [
        "# implement your attack here\n",
        "@torch.no_grad()\n",
        "def your_attack(\n",
        "\n",
        "):\n",
        "    pass\n",
        "\n",
        "scores_test = your_attack(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe128e",
      "metadata": {
        "id": "59fe128e"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ae9279c",
      "metadata": {
        "id": "0ae9279c"
      },
      "outputs": [],
      "source": [
        "# load the label here to compute the performance, you will only have full access to the label in phase 1 as a warm-up\n",
        "label_path = data_dir / \"test_label.json\"\n",
        "label_items = _read_json(label_path)\n",
        "\n",
        "y_true = np.array(label_items)\n",
        "scores = np.array(scores_test)\n",
        "fpr, tpr, thr = roc_curve(y_true, scores)\n",
        "auc_val = roc_auc_score(y_true, scores)\n",
        "print(auc_val) # area under curve\n",
        "\n",
        "# some tpr fpr\n",
        "print(max(tpr[fpr < 0.01])) # TPR @ 0.01FPR\n",
        "print(max(tpr[fpr < 0.05])) # TPR @ 0.05FPR\n",
        "# report this three metrics in your report!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2e08d5",
      "metadata": {
        "id": "de2e08d5"
      },
      "outputs": [],
      "source": [
        "# draw ROC curve and attach the figure in the report\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_val:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance line')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title(f'MIA ROC Curve for Train Data')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}